{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d25f7e9a",
   "metadata": {},
   "source": [
    "# IMPORT PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18d0ea0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-12 20:34:30.957577: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/luke/ws_moveit/devel/lib\n",
      "2022-09-12 20:34:30.957626: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import math\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from numpy import matrix \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import model_selection\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow import keras\n",
    "from kerastuner.tuners import RandomSearch\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import matrix\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "import numpy as np\n",
    "from scipy import interp\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from pandas import DataFrame\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import kerastuner as kt\n",
    "from pandas import DataFrame\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import model_selection\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras import callbacks\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints import maxnorm\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    MaxPooling2D\n",
    ")\n",
    "from kerastuner.tuners import Hyperband\n",
    "\n",
    "from pandas import DataFrame\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import model_selection\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import kerastuner as kt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03739f0",
   "metadata": {},
   "source": [
    "# READ MATRICES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd8395c",
   "metadata": {},
   "source": [
    "Pearson's connection matrix of the EEG experiments of subjects who ingested Ayahuasca at the time after the psychedelic activation time can be found in: Alves, Caroline (2022): With-ayahuasca. figshare. Dataset. https://doi.org/10.6084/m9.figshare.21082513.v1\n",
    "\n",
    "Pearson's connection matrix of the EEG experiments of subjects who ingested Ayahuasca at the time before the psychedelic activation time: Alves, Caroline (2022): No-ayahuasca. figshare. Dataset.\n",
    "https://doi.org/10.6084/m9.figshare.21082531.v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf11057",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define function to read\n",
    "def pixels_from_path(file_path):\n",
    "    im = open(file_path)\n",
    "    test = pd.read_csv(im,sep=\",\", index_col=False,header=None)\n",
    "    #im = im.resize(IMG_SIZE)\n",
    "    np_im = np.array(test)\n",
    "    #matrix of pixel RGB values\n",
    "    return np_im\n",
    "\n",
    "print(\"with ayahuasca\")\n",
    "#insert path for Pearson connectivity matrix with ayahuasca\n",
    "aya = np.asarray([pixels_from_path(n) for n in glob.glob(r'PATH-ayahuasca/*csv')])\n",
    "print(\"shape for ayahuasca matrices\")\n",
    "print(np.shape(aya ))\n",
    "#insert path for Pearson connectivity matrix no ayahuasca\n",
    "print(\"no ayahuasca\")\n",
    "normal = np.asarray([pixels_from_path(n) for n in glob.glob(r'aPATH-no-yahuasca/*csv')])\n",
    "print(normal)\n",
    "print(\"shape for no ayahuasca matrices\")\n",
    "print(np.shape(normal))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672b3e41",
   "metadata": {},
   "source": [
    "# PREPARING TRAIN SET AND LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75117570",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preparing train set\n",
    "x_train = np.concatenate([normal ,aya])\n",
    "x=np.nan_to_num(x_train) \n",
    "print(\"shape for train set\")\n",
    "print(np.shape(x_train))\n",
    "\n",
    "## Preparing test set\n",
    "yes_l = [1]*len(aya)\n",
    "no_l = [0]*len(normal)\n",
    "labels = yes_l + no_l\n",
    "#print(labels) ## the labels of the graphs\n",
    "y=np.nan_to_num(labels)\n",
    "print(\"length for test set\")\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d8845a",
   "metadata": {},
   "source": [
    "# DEFINE CNN MODEL AND TUNNING HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cce578",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "]\n",
    "\n",
    "\n",
    "class CNNHyperModel(HyperModel):\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def build(self, hp):\n",
    "        model = keras.Sequential()\n",
    "        model.add(\n",
    "            Conv2D(\n",
    "                filters=16,\n",
    "                kernel_size=3,\n",
    "                activation='relu',\n",
    "                input_shape=self.input_shape\n",
    "            )\n",
    "        )\n",
    "        model.add(\n",
    "            Conv2D(\n",
    "                filters=16,\n",
    "                activation='relu',\n",
    "                kernel_size=3\n",
    "            )\n",
    "        )\n",
    "        model.add(MaxPooling2D(pool_size=2))\n",
    "        model.add(\n",
    "            Dropout(rate=hp.Float(\n",
    "                'dropout_1',\n",
    "                min_value=0.01,\n",
    "                max_value=0.5,\n",
    "                default=0.25,\n",
    "                step=0.05,\n",
    "            ))\n",
    "        )\n",
    "        model.add(\n",
    "            Conv2D(\n",
    "                filters=32,\n",
    "                kernel_size=3,\n",
    "                activation='relu'\n",
    "            )\n",
    "        )\n",
    "        model.add(\n",
    "            Conv2D(\n",
    "                filters=hp.Choice(\n",
    "                    'num_filters',\n",
    "                    values=[32, 64],\n",
    "                    default=64,\n",
    "                ),\n",
    "                activation='relu',\n",
    "                kernel_size=3\n",
    "            )\n",
    "        )\n",
    "        model.add(MaxPooling2D(pool_size=2))\n",
    "        model.add(\n",
    "            Dropout(rate=hp.Float(\n",
    "                'dropout_2',\n",
    "                min_value=0.05,\n",
    "                max_value=0.5,\n",
    "                default=0.25,\n",
    "                step=0.05,\n",
    "            ))\n",
    "        )\n",
    "        model.add(Flatten())\n",
    "        model.add(\n",
    "            Dense(\n",
    "                units=hp.Int(\n",
    "                    'units',\n",
    "                    min_value=32,\n",
    "                    max_value=512,\n",
    "                    step=32,\n",
    "                    default=128\n",
    "                ),\n",
    "                activation=hp.Choice(\n",
    "                    'dense_activation',\n",
    "                    values=['relu', 'tanh', 'sigmoid'],\n",
    "                    default='relu'\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        model.add(\n",
    "            Dropout(\n",
    "                rate=hp.Float(\n",
    "                    'dropout_3',\n",
    "                    min_value=0.001,\n",
    "                    max_value=0.5,\n",
    "                    default=0.25,\n",
    "                    step=0.05\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        model.add(Dense(self.num_classes, activation='softmax'))\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(\n",
    "                hp.Float(\n",
    "                    'learning_rate',\n",
    "                    min_value=1e-4,\n",
    "                    max_value=1e-2,\n",
    "                    sampling='LOG',\n",
    "                    default=1e-3\n",
    "                )\n",
    "            ),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=METRICS\n",
    "        )\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a8fc4a",
   "metadata": {},
   "source": [
    "# RESAMPLING AND CALLING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a60c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "NUM_CLASSES = 2  #\n",
    "HYPERBAND_MAX_EPOCHS = 100\n",
    "MAX_TRIALS = 100\n",
    "EXECUTION_PER_TRIAL = 2\n",
    "INPUT_SHAPE = (63, 63, 1) \n",
    "#X_train, X_test, Y_train, Y_test = train_test_split(x, y, stratify=y, test_size=0.10)\n",
    "skfold = StratifiedKFold(n_splits=10)\n",
    "for train_index, test_index in skfold.split(X_train,Y_train):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    x_train, x_test = X_train[train_index], X_train[test_index]\n",
    "    y_train, y_test = Y_train[train_index], Y_train[test_index]\n",
    "    print(np.shape(x_train))\n",
    "    \n",
    "    y_train = np_utils.to_categorical(y_train)\n",
    "    y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "    #count_classes = y_test.shape[1]\n",
    "    x_test = x_test.reshape(-1, 63, 63, 1)\n",
    "    x_train = x_train.reshape(-1, 63, 63, 1)\n",
    "    \n",
    " \n",
    "\n",
    "    hypermodel = CNNHyperModel(input_shape=INPUT_SHAPE, num_classes=NUM_CLASSES)\n",
    "    tuner = RandomSearch(\n",
    "    hypermodel,\n",
    "    objective='val_accuracy',\n",
    "    seed=SEED,\n",
    "    max_trials=MAX_TRIALS,\n",
    "    executions_per_trial=EXECUTION_PER_TRIAL,\n",
    "   \n",
    "     \n",
    "    overwrite = True  \n",
    "\n",
    "    )\n",
    "    tuner.search(x_train, y_train, epochs=100,  validation_data=(x_test,y_test), verbose=0)\n",
    "    best_model_rs = tuner.get_best_models(num_models=1)[0]\n",
    "#tuner.results_summary()\n",
    "# Retrieve the best model.\n",
    "    best_model_rs = tuner.get_best_models(num_models=1)[0]\n",
    "    history_rs=best_model_rs.fit(x_train,y_train, epochs=100, validation_data=(x_test,y_test))\n",
    "    print(best_model_rs.evaluate(x_train, y_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6b7d1a",
   "metadata": {},
   "source": [
    "# PRINT RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58df174",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Print results for train set\")\n",
    "best_model_rs.evaluate(x_train, y_train)\n",
    "X_test = X_test.reshape(-1, 63, 63, 1)\n",
    "Y_test = to_categorical(Y_test)\n",
    "print(\"Print results for test set\")\n",
    "best_model_rs.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1334694",
   "metadata": {},
   "source": [
    "# PLOT ROC CURVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca44540",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score= best_model_rs.predict(X_test)\n",
    "# Plot linewidth.\n",
    "lw = 2\n",
    "n_classes=2\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(Y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(Y_test.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.figure(1)\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='slategray', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['purple', 'lightseagreen'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--',color='#cb416b', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.annotate(' Random Guess',(.5,.48),color='#cb416b')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "#plt.title('Receiver operating characteristic obtained by the model found by the Random Search algorithm \\n for the Granger Connectivity Matrices for Alzheimer database ')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f262bb93",
   "metadata": {},
   "source": [
    "# PLOT CONFUSION MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543b9e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score= best_model_rs.predict(X_test)\n",
    "y_score=np.argmax(y_score, axis=1)\n",
    "Y_test=np.argmax(Y_test, axis=1)\n",
    "cm = metrics.confusion_matrix(y_score, Y_test)\n",
    "cm_sum = np.sum(cm, axis=1)\n",
    "cm_perc = cm / cm_sum.astype(float) * 100\n",
    "annot = np.empty_like(cm).astype(str)\n",
    "nrows, ncols = cm.shape\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "        c = cm[i, j]\n",
    "        p = cm_perc[i, j]\n",
    "        if i == j:\n",
    "            s = cm_sum[i]\n",
    "            annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n",
    "        elif c == 0:\n",
    "            annot[i, j] = ''\n",
    "        else:\n",
    "            annot[i, j] = '%.1f%%\\n%d' % (p, c)\n",
    "cm = pd.DataFrame(cm)\n",
    "cm.index.name = 'Actual'\n",
    "cm.columns.name = 'Predicted'\n",
    "#fig, ax = plt.subplots(figsize=figsize)\n",
    "sns.heatmap(cm, annot=annot, fmt='',cmap='rocket_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42fd9c0",
   "metadata": {},
   "source": [
    "# PLOT LEARNING CURVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b9c8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "acc = history_rs.history['accuracy']\n",
    "val_acc =history_rs.history['val_accuracy']\n",
    "loss = history_rs.history['loss']\n",
    "val_loss = history_rs.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
